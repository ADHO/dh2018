<?xml version="1.0" encoding="UTF-8"?>
<html xml:lang="en" xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
    <!--THIS FILE IS GENERATED FROM AN XML MASTER. DO NOT EDIT (5)-->
    <title>Quechua Real Words: An Audiovisual Corpus of Expressive Quechua Ideophones</title>
    <meta name="author" content="Jeremy Browne and Janis Nuckolls"/>
    <meta name="generator" content="Text Encoding Initiative Consortium XSLT stylesheets"/>
    <meta name="DC.Title" content="Quechua Real Words: An Audiovisual Corpus of Expressive Quechua Ideophones"/>
    <meta name="DC.Type" content="Text"/>
    <meta name="DC.Format" content="text/html"/>
    <link href="http://www.tei-c.org/release/xml/tei/stylesheet/tei.css" rel="stylesheet" type="text/css"/>
    <link rel="stylesheet" media="print" type="text/css" href="http://www.tei-c.org/release/xml/tei/stylesheet/tei-print.css"/>
  </head>
  <body class="simple" id="TOP">
    <div class="stdheader autogenerated">
      <h1 class="maintitle">Quechua Real Words: An Audiovisual Corpus of Expressive Quechua Ideophones</h1>
    </div>
    <div class="dhconvalidator-xml-link">
      <a href="BROWNE_Jeremy_Quechua_Real_Words__An_Audiovisual_Corpus_of_E.xml">XML</a>
    </div>
    <!--TEI front-->
    <!--TEI body-->
    <p>Introduction</p>
    <p>Ideophones, sometimes called “mimetics” (Akita, 2009) or “expressives” (Diffloth, 1976) are expressions that communicate sensory aspects of the physical word such as sound (i.e., onomatopoeia), movement, color, etc., or cognitive/emotional states (e.g., “ta-da” in English). Although most linguistics description of and inquiry into ideophones have focused on vocal expressions, gestures are integrated with ideophonic utterances in some languages. The analysis of these gestures and their symbolism may augment scholars’ understanding of the target language including how native speakers mentally represent their environment.</p>
    <p>In this paper, we describe a web-based tool, 
      <span style="font-style:italic">Quechua Real Words</span>, used by ideophonic linguists at [institution] to catalog and study multimedia representations of gestured ideophones as performed by native speakers of Pastaza Quichua. Research based on this tool is opening new understanding of the target language’s aesthetics, especially regarding the non-arbitrariness of gestured signs. We also discuss the relationship between this tool and other digital humanities efforts.
    </p>
    <p>Project Background</p>
    <p>The indigenous people of eastern Ecuador speak Pastaza Quichua (PQ), a dialect of Northern Quechua. Descended from the language of the Incan civilization, Quechua is still spoken by as many as 10 million people in the Andes region stretching from Ecuador in the north to Argentina in the south. In 2015 [second author], a linguistic professor at [institution] led a group of student researchers who spent one semester in Ecuador recording and appreciating the indigenous culture and language. This team videoed over a hundred hours of interviews with PQ speakers, including thousands of examples of PQ ideophonic gestures.</p>
    <p>The team returned to [institution] baffled at the scope of archival work that lay between their raw footage and their research goals. In consultation with their [institution]’s Office of Digital Humanities, they constructed a WordPress-based website that facilitated their archival activities, accelerated their research, and opened their work to a global audience.</p>
    <p>The Website</p>
    <p>
      <span style="font-style:italic">Quechua Real Words</span> uses custom content types within WordPress and simple data entry forms that allow students and professors with little computer experience to record ideophones and link entries to specific segments of recorded videos. The project’s footage is hosted on YouTube for simplicity and accessibility, and the data entry form only requests the video segment’s URL and start and stop times.
    </p>
    <div class="figure">
      <img src="Pictures/69299c697481fad1173e3d9ebaaf7274.jpeg" alt="" class="inline" style=" width:15.980833333333333cm; height:13.828888888888889cm;"/>
    </div>
    <p>
      <span style="font-style:italic">Quechua Real Words</span> ideophone recording form.
    </p>
    <p>The entry form includes two other important features: First, the researcher may classify each ideophone by one or more “sensory modality” (e.g., color, haptic, movement, etc.). Second, each scholar—be they professor or student—may add their name to the list of the entry’s contributors.</p>
    <p>Once an ideophone is saved, it immediately appears on two indexes: the list of all ideophones, and the list of ideophones by modality. The first index allows researchers to look up specific ideophones, while the second promotes synthetic exploration where relationships between apparently unrelated ideophones can be made clear.</p>
    <p>Each ideophone page displays the pronunciation (in IPA format), definition and other information one would expect from a traditional dictionary entry. It also shows a text description of the ideophone’s paralinguistic qualities and one or more videos of native speakers expressing the ideophone in candid conversation. These videos are segments of longer YouTube videos, and the segments may be looped, paused, and replayed. (Such functionality is not native to YouTube’s standard embedded player, so the site’s video player is a custom JavaScript that connects to YouTube’s published API.) </p>
    <div class="figure">
      <img src="Pictures/f352967faf06248adc79eefb7ac091e0.jpeg" alt="" class="inline" style=" width:15.980833333333333cm; height:14.675555555555556cm;"/>
    </div>
    <p>An ideophone page from 
      <span style="font-style:italic">Quechua Real Words</span>.
    </p>
    <p>As insisted on by the supervising professor, each ideophone page displays a “How to Cite” section with a citation in the Linguistics Society of America’s preferred format. To recognize the collaborative nature of the website, the credited parties in the citation include everyone who contributed to the entry, even students. </p>
    <p>Research Potential</p>
    <p>During the first two years of its existence, [first author] used 
      <span style="font-style:italic">Quechua Real Words</span> for research published in a special issue of the 
      <span style="font-style:italic">Canadian Journal of Linguistics</span> ([second author], 2017), in three presentations at international conferences ([second author], 2015a; [second author], 2015b; [second author], 2014), and in two invited book chapters ([second author], in press; [second author], in press). Additionally, the website’s content will inform an upcoming monograph ([second author], in preparation).
    </p>
    <p>These publications focus on contextually-rich methods of understanding PQ ideophones, comparing specific gestures and intonations between speakers and contexts, and discovering how the ideophones are integrated with—rather than distinct from—the language’s verbal aspects. As Akita and Tsujimura (2016) point out, the goal is to seek typological generalizations for ideophones rather than consider them in isolation. [Second author] seeks to extend these integrative studies and semantic generalizations beyond the vocal utterances into the gestured space.</p>
    <p>Quechua Real Words as a Model for DH Collaboration</p>
    <p>When [second author] proposed this website to the Office of Digital Humanities, [she/he] had little notion that it would lead to such a level of scholarly productivity. It was only as [she/he] saw how the site could function that [she/he] began to grasp its potential. Similarly, [first author], the digital humanists who crafted the website, overlooked its potential because, quite frankly, the technology behind 
      <span style="font-style:italic">Quechua Real Words </span>is rudimentary for most DH centers.
    </p>
    <p>Perhaps [first author]’s estimation was clouded by the fact that DH as a field has favored text-based literary analysis over multimedia research. Despite the work of the ARTeFACT project (Coartney &amp; Wiesner, 2009) and a few others who have considered digital analysis of performing arts, DH has contributed much less to the analysis of video interactions, such as these ideophones, than it has to the analysis of written text. Garrard, Haigh, and de Jager (2011) demonstrate the status-quo for dealing with nonverbal communication in DH research: “…the recording and representation of various types of paralinguistic feature in transcription is somewhat idiosyncratic, and thus unreliable, suggesting that they should be removed in the interests of consistency.”</p>
    <p>This lack of emphasis on paralinguistic and nonverbal communication is in spite of those features’ apparent value. “The nonverbal channel carries important information about emotional expressions… Systems that combine multiple modalities usually outperform single-modality systems in recognizing emotional” (Truong, Westerhof, Lamers, &amp; de Jong, 2014). Unfortunately, even Truong et al. restricted their valuation of nonverbal channels to prosodic qualities such as timing and rhythm; they did not address issues of body language or gestures.</p>
    <p>Regardless of why [first author] overlooked the website’s potential, [she/he] has since changed how [she/he] evaluates potential collaborative DH projects. [She/He] now focuses on evaluating the use of the tools, websites, and other resources [she/he] would develop 
      <span style="font-style:italic">relative to the target discipline</span> rather than relative to the state of the art within DH. This new approach has already proven fruitful (first author, 2017).
    </p>
    <p>Future Plans</p>
    <p>While [second author] continues to leverage 
      <span style="font-style:italic">Quechua Real Words</span> for [her/his] scholarship, [first author] has combed the DH literature to discover methods of extending the site’s capacity. One DH project that could contribute guidance to this project is the work of Paquette-Bigras and Forest (2014) who attempted to build a descriptive vocabulary for dance movements. A similar effort to construct a vocabulary for describing non-vocal expressions may reveal yet-unnoticed relationships between expressive gestures. This would require intense, non-automated markup of the gestures, but the 
      <span style="font-style:italic">Quechua Real Words</span> website and the student-involved structure of [second author]’s courses would be facilitative. Such detailed modeling of the gestures would extend the modality-based clustering currently available on the website to include form-based clustering of the gestures.
    </p>
    <p>Additionally, we are working with [institution’s library] to add 
      <span style="font-style:italic">Quechua Real Words</span> to their federated search databases. This will increase the site’s discoverability by scholars and students throughout the world.
    </p>
    <!--TEI back-->
    <div class="bibliogr" id="index.xml-back.1_div.1">
      <h2>
        <span class="headingNumber">Appendix A </span>
      </h2>
      <div class="listhead">Bibliography</div>
      <ol class="listBibl">
        <li id="index.xml-bibl-w363520aab3b3b1b1b3">
          <div class="biblfree">Akita, K. 2009. 
            <span style="font-style:italic">A grammar of sound-symbolic words in Japanese: Theoretical approaches to iconic and indexical properties of mimetics</span>. PhD Dissertation. Kobe University.
          </div>
        </li>
        <li id="index.xml-bibl-w363520aab3b3b1b1b5">
          <div class="biblfree">Akita, K. &amp; Tsujimura, N. 2016. “Mimetics”. In T. Kageyama and H. Kishimoto (eds), 
            <span style="font-style:italic">Handbook of Japanese Lexicon and Word Formation,</span> 133–160. Berlin: Gruyter De Mouton.
          </div>
        </li>
        <li id="index.xml-bibl-w363520aab3b3b1b1b7">
          <div class="Reference">Coartney, J. S. &amp; Wiesnet, S. L. (2009). Performance as digital text: Capturing signals and secret messages in a media-rich experience. 
            <span style="font-style:italic">Literary and Linguistic Computing, 24</span>(2), pp. 153–160. https://doi.org/10.1093/llc/fqp012
          </div>
        </li>
        <li id="index.xml-bibl-w363520aab3b3b1b1b9">
          <div class="biblfree">Diffloth, G. 1976. “Expressives in Semai” 
            <span style="font-style:italic">Oceanic Linguistics Special Publications</span>
          </div>
        </li>
        <li id="index.xml-bibl-w363520aab3b3b1b1c11">
          <div class="biblfree">No. 13, 
            <span style="font-style:italic">Austroasiatic Studies Part I</span>, pp. 249-264
          </div>
        </li>
        <li id="index.xml-bibl-w363520aab3b3b1b1c13">
          <div class="Reference">Garrad, P., Haigh, A., &amp; de Jager, C. (2011). Techniques for transcribers: assessing and improving consistency in transcripts of spoken language. Literary and Linguistic Computing, 26(4), pp. 389–405. https://doi.org/10.1093/llc/fqr018</div>
        </li>
        <li id="index.xml-bibl-w363520aab3b3b1b1c15">
          <div class="Reference">Paquette-Bigras, E. &amp; Forest, D. (2014). A Vocabulary of the Aesthetic Experience for Modern Dance Archives. Paper presented at DH 2014, Lausanne, Switzerland.</div>
        </li>
        <li id="index.xml-bibl-w363520aab3b3b1b1c17">
          <div class="Reference">Truong, K. P., Westerhof, G. J., Lamers, S. M. A., de Jong, F. (2014). Towards modeling expressed emotions in oral history interviews: Using verbal and nonverbal signals to track personal narratives. Literary and Linguistic Computing, 29(4), pp. 621–636. https://doi.org/10.1093/llc/fqu041</div>
        </li>
        <li id="index.xml-bibl-w363520aab3b3b1b1c19">
          <div class="Reference">[The following references will be added following double-blind review:] 
            <br/>[first author], 2017 
            <br/>[second author], 2014 
            <br/>[second author], 2015a 
            <br/>[second author], 2015b 
            <br/>[second author], 2017 
            <br/>[second author], in press 
            <br/>[second author], in press 
            <br/>[second author], in preparation
          </div>
        </li>
      </ol>
    </div>
    <div class="stdfooter autogenerated">
      <address>Jeremy Browne (jeremy_browne@byu.edu), Brigham Young University, United States of America y Janis Nuckolls (cvd6262@gmail.com), Brigham Young University, United States of America</address>
    </div>
  </body>
</html>
