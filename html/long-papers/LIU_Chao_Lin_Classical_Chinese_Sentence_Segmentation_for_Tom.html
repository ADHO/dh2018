<?xml version="1.0" encoding="UTF-8"?>
<html xml:lang="en" xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
    <!--THIS FILE IS GENERATED FROM AN XML MASTER. DO NOT EDIT (5)-->
    <title>Classical Chinese Sentence Segmentation for Tomb Biographies of Tang Dynasty</title>
    <meta name="author" content="Chao-Lin Liu and Yi Chang"/>
    <meta name="generator" content="Text Encoding Initiative Consortium XSLT stylesheets"/>
    <meta name="DC.Title" content="Classical Chinese Sentence Segmentation for Tomb Biographies of Tang Dynasty"/>
    <meta name="DC.Type" content="Text"/>
    <meta name="DC.Format" content="text/html"/>
    <link href="http://www.tei-c.org/release/xml/tei/stylesheet/tei.css" rel="stylesheet" type="text/css"/>
    <link rel="stylesheet" media="print" type="text/css" href="http://www.tei-c.org/release/xml/tei/stylesheet/tei-print.css"/>
  </head>
  <body class="simple" id="TOP">
    <div class="stdheader autogenerated">
      <h1 class="maintitle">Classical Chinese Sentence Segmentation for Tomb Biographies of Tang Dynasty</h1>
    </div>
    <div class="dhconvalidator-xml-link">
      <a href="LIU_Chao_Lin_Classical_Chinese_Sentence_Segmentation_for_Tom.xml">XML</a>
    </div>
    <!--TEI front-->
    <!--TEI body-->
    <div class="DH-Heading1" id="index.xml-body.1_div.1">
      <h2 class="DH-Heading1">
        <span class="headingNumber">1. </span>
        <span class="head">Introduction</span>
      </h2>
      <p>Figure 1 shows a slab of tomb biography of the Tang dynasty. 
        <span id="ftn1_return">
          <a class="notelink" title="This image was downloaded from &lt;http://www.lyqtzz.com/uploadfile/20110817165325665.jpg&gt;. The Tang dynasty existed between 688CE and 907CE. More images…" href="#ftn1">
            <sup>1</sup>
          </a>
        </span> Researchers can copy the words on such slabs to produce a collection of tomb biographies for research. A typical tomb biography contains various types of information about the deceased and their families and, sometimes, a rhyming passage of admiration. Employing software tools to analyze the texts, one can extract useful information from the collections of tomb biographies to enrich databases like the China Biographical Database (CBDB) to support further Chinese studies. 
        <span id="ftn2_return">
          <a class="notelink" title="The China Biographical Database (https://projects.iq.harvard.edu/cbdb/home) is a free and open database for Chinese studies." href="#ftn2">
            <sup>2</sup>
          </a>
        </span>
      </p>
      <div class="figure">
        <img src="Pictures/d62bcaf6d41fdcce7d7a82798561fd12.jpg" alt="" class="inline" style=" width:9.6774cm; height:8.466666666666667cm;"/>
      </div>
      <p>
        <span style="font-weight:bold">Figure 1. A slab of tomb biography of the Tang dynasty</span>
      </p>
      <p>It is well known that modern Chinese texts do not include delimiters like spaces to separate words. Hence, researchers design algorithms for segmenting Chinese character strings into words (Sun et al., 2004; Shao et al., 2017). </p>
      <p>In contrast, it is not as well known that, in classical Chinese, there were no markers for the separation of sentences. The characters in Figure 1 simply connect to each other. In modern Chinese, texts are punctuated for pauses in sentences and ends of sentences. The research about algorithmically inserting these syntactic markers into classical Chinese is receiving more attention along with the growth of digital humanities in recent years. The needs of segmenting ancient texts for humanities studies are not unique to Chinese studies, interested readers can find examples for German texts (Petran, 2012) and Swedish texts (Bouma and Adesam, 2013).</p>
      <p>Huang et al. 
        <a id="Ref499278733">
          <!--anchor-->
        </a> (2010) employed the techniques of conditional random fields (CRFs) to segment texts of literature and history. They achieved 0.7899 and 0.9179 in F 
        <sub>1</sub>
        <a id="Ref499189299">
          <!--anchor-->
        </a>, 
        <span id="ftn3_return">
          <a class="notelink" title="The precision rate, recall rate, and F measure are designed for evaluating the effectiveness of information retrieval and extraction. F 1 is a popular…" href="#ftn3">
            <sup>3</sup>
          </a>
        </span> respectively, for segmenting the texts in Shiji and Zuozhuan. 
        <span id="ftn4_return">
          <a class="notelink" title="Shiji (史記) and Zuozhuan (左傳) are two very important sources about Chinese history." href="#ftn4">
            <sup>4</sup>
          </a>
        </span> Wang et al. (2016; 2017) applied recurrent neural networks to segment texts in a diverse collection of classical Chinese sources. They achieved F 
        <sub>1</sub> measures that are close to 0.75, and item accuracies that are near 0.91. 
        <span id="ftn5_return">
          <a class="notelink" title="The item accuracy evaluates the labeling judgments including both punctuated and non-punctuated items. In a typical sentence segmentation task, there …" href="#ftn5">
            <sup>5</sup>
          </a>
        </span> The researchers achieved different segmentation results for different corpora even when they applied the same techniques and procedures. It is thus inappropriate to just compare the numbers for ranking because the nature of the corpora varies widely.
      </p>
      <p>In this proposal, we report our attempts to segment texts in tomb biographies with CRF models (Lafferty, 2001). We studied the effects of considering different types of lexical information in the models, and achieved 0.853 in precision, 0.807 in recall, 0.829 in F 
        <sub>1</sub>, and 0.940 in item accuracy 
        <a id="Ref499362493">
          <!--anchor-->
        </a>. 
        <span id="ftn6_return">
          <a class="notelink" title="We interviewed Hongsu Wang (王宏甦), the project manager of the China Biographical Database Project at Harvard University, about his preferences in post-…" href="#ftn6">
            <sup>6</sup>
          </a>
        </span> Better results were accomplished when we employed deep learning techniques, including applicaitons of long short-term memory networks and sequence-to-sequence networks, for segmenting our tomb biographies.
      </p>
    </div>
    <div class="DH-Heading1" id="index.xml-body.1_div.2">
      <h2 class="DH-Heading1">
        <span class="headingNumber">2. </span>
        <span class="head">Data Sources</span>
      </h2>
      <p>We obtained digitized texts for three books of tomb biographies of the Tang dynasty (Zhou and Zhao, 1992; 2001). The collection consists of 5119 biographies which contain 423,922 periods, commas, and semicolons. There are 5505 distinct types of characters and a total of more than 2461 thousand of characters in the collection. 
        <span id="ftn7_return">
          <a class="notelink" title="In terms of Linguistics, we have 5505 character types and 2,461,000 character tokens." href="#ftn7">
            <sup>7</sup>
          </a>
        </span> When counting these statistics, we ignored a very small portion of characters that cannot be shown without special fonts. Hence, these statistics are not perfectly precise, but they are accurate within a reasonable range. On average, a biography has about 480 characters. Some of them are very short and have many missing characters. Hence, we exclude biographies that have no more than 30 characters in our experiments. 
        <span id="ftn8_return">
          <a class="notelink" title="30 is an arbitrary choice, and can be changed easily." href="#ftn8">
            <sup>8</sup>
          </a>
        </span>
      </p>
    </div>
    <div class="DH-Heading1" id="index.xml-body.1_div.3">
      <h2 class="DH-Heading1">
        <span class="headingNumber">3. </span>
        <span class="head">Training and Testing CRF Models</span>
      </h2>
      <p>We consider the segmentation task as a classification problem. Let C 
        <sub>i</sub> denote an individual character in the texts. We categorize each character as either 
        <span style="font-weight:bold;color:0000FF">M</span> (for “followed by a punctuation mark”) and 
        <span style="font-weight:bold;color:0000FF">O</span> (for “an ordinary character”). Assume that we should add only a punctuation mark between C 
        <sub>3</sub> and C 
        <sub>4</sub> for a string “C 
        <sub>1</sub> C 
        <sub>2</sub> C 
        <sub>3</sub> C 
        <sub>4</sub> C 
        <sub>5</sub>”. A correct labeling for this string will be “O O M O O”. 
        <span id="ftn9_return">
          <a class="notelink" title="Due to the constraint on the word count in DH 2018 proposals, we can only briefly outline the steps for training and testing CRF models. More details …" href="#ftn9">
            <sup>9</sup>
          </a>
        </span>
      </p>
      <p>We can convert each character in the texts into an 
        <span style="font-weight:bold;color:0000FF">instance</span>, which may be used for training or testing. We provide with each instance a group of contextual 
        <span style="font-weight:bold;color:0000FF">features</span> that may be relevant to the judgment of whether or not a punctuation mark is needed. For instance, we may use one character surrounding a character X and itself as the group of features. The following are two instances that we create for C 
        <sub>3</sub> and C 
        <sub>4</sub>. The instance for C 
        <sub>3</sub> is (1), and the leftmost item is the correct label for C 
        <sub>3</sub>, and the rest are the features for C 
        <sub>3</sub>. 
        <span id="ftn10_return">
          <a class="notelink" title="Here, we adopt typical notations for CRF-based applications. w[0] is the current word, w[-1] is the neighbor word to the left of the current word, w[1…" href="#ftn10">
            <sup>10</sup>
          </a>
        </span>
      </p>
      <p>M w[0]=C 
        <sub>3</sub>,w[-1]=C 
        <sub>2</sub>,w[1]=C 
        <sub>4</sub> (1)
      </p>
      <p>O w[0]=C 
        <sub>4</sub>,w[-1]=C 
        <sub>3</sub>,w[1]=C 
        <sub>5</sub> (2)
      </p>
      <p>We can train a CRF model with a selected portion of the instances (called 
        <span style="font-weight:bold;color:0000FF">training data</span>), and test the resulting model with the remaining instances (called 
        <span style="font-weight:bold;color:0000FF">test data</span>). The instances in the training and the test data are mutually exclusive.
      </p>
      <p>We employ a machine-learning tool that learns from the training data to build a CRF model. 
        <span id="ftn11_return">
          <a class="notelink" title="CRFSuite: &lt;http://www.chokkan.org/software/crfsuite/&gt;" href="#ftn11">
            <sup>11</sup>
          </a>
        </span> We then apply the learned model to predict the classes of the instances in the test data. The labels of the instances in the test data are temporarily concealed when the learned model makes predictions. 
        <span id="ftn12_return">
          <a class="notelink" title="Thus, the instances for testing CRFs look like (1) and (2) that do not carry the correct labels, M and O, respectively.." href="#ftn12">
            <sup>12</sup>
          </a>
        </span> The precision rate and recall rate of the learned model are calculated with the correct and the predicted labels.
      </p>
      <p>We report four sets of basic experiments next, each investigating an important aspect for analyzing Chinese texts. The 5119 biographies were resampled and randomly assigned to the training (70%) and test (30%) sets for every experiment. 
        <span id="ftn13_return">
          <a class="notelink" title="Recall that we used only those biographies that have no less than 30 characters." href="#ftn13">
            <sup>13</sup>
          </a>
        </span> We repeated every experiment three times, and report the averages of the precision and recall rates.
      </p>
      <div class="DH-Heading2" id="index.xml-body.1_div.3_div.1">
        <h3 class="DH-Heading2">
          <span class="headingNumber">3.1. </span>
          <span class="head">Changing the Size of the Context</span>
        </h3>
        <p>We certainly can and should consider more than one character around the current character as the context. Figure 2 shows the test results of using different sizes of contexts for the instances. The horizontal axis shows the sizes, e.g., when 
          <span style="font-style:italic">k</span>=2, the feature set includes information about two characters on both sides of the current character. P1 and R1 are the average precision and recall rates, respectively.
        </p>
        <div class="figure">
          <img src="Pictures/4105c6ba934289941f2453a8d1a1fcb0.jpg" alt="" class="inline" style=" width:14.9cm; height:7cm;"/>
        </div>
        <p>
          <span style="font-weight:bold">Figure 2. Effects of varying context sizes</span>
        </p>
        <p>We expected to improve the precision and recall rates by expanding the width of the context. The margin of improvements gradually decreased, and the curves level off after the window sizes reached six. The recall rises sharply when we add the immediate neighbor word into the features, emphasizing the predicting power of the immediate neighbor character. When 
          <span style="font-style:italic">k</span>=10, the precision and recall are 0.765 and 0.729, respectively, and the item accuracy exceeds 0.91.
        </p>
      </div>
      <div class="DH-Heading2" id="index.xml-body.1_div.3_div.2">
        <h3 class="DH-Heading2">
          <span class="headingNumber">3.2. </span>
          <span class="head">Adding Bigrams</span>
        </h3>
        <p>We added bigrams that were formed by consecutive characters into the features. The following instance shows the result of adding bigrams to the features in (1). 
          <span id="ftn14_return">
            <a class="notelink" title="Here, w[-1_0] is the bigram on the left side of the current word, and w[0_1] is the bigram to the right of the current word. When we consider bigrams …" href="#ftn14">
              <sup>14</sup>
            </a>
          </span>
        </p>
        <p>M w[0]=C 
          <sub>3</sub>,w[-1]=C 
          <sub>2</sub>,w[1]=C 
          <sub>4</sub>, 
          <span style="font-weight:bold;color:0000FF">w[-1_0]=C</span>
          <sub style="font-weight:bold;color:0000FF">2</sub>
          <span style="font-weight:bold;color:0000FF">C</span>
          <sub style="font-weight:bold;color:0000FF">3</sub>
          <span style="font-weight:bold;color:0000FF">,w[0_1]=C</span>
          <sub style="font-weight:bold;color:0000FF">3</sub>
          <span style="font-weight:bold;color:0000FF">C</span>
          <sub style="font-weight:bold;color:0000FF">4</sub> (3)
        </p>
        <p>Figure 3 shows the test results of adding bigrams while we also tried different sizes of context. The curves named P1 and R1 are from Figure 2, and P2 and R2 are results achieved by adding bigrams to the features. Both rates are improved, and the gains are remarkable. </p>
        <div class="figure">
          <img src="Pictures/3473fbfc3acf4b4a075c18245db762b1.jpg" alt="" class="inline" style=" width:14.82cm; height:7cm;"/>
        </div>
        <p>
          <span style="font-weight:bold">Figure 3. Adding bigrams improves the results.</span>
        </p>
      </div>
      <div class="DH-Heading2" id="index.xml-body.1_div.3_div.3">
        <h3 class="DH-Heading2">
          <span class="headingNumber">3.3. </span>
          <span class="head">Effects of Pronunciation Information</span>
        </h3>
        <p>Using the characters and their bigrams in the features is an obvious requirement. Since the tomb biographies may contain rhyming parts, it is also intriguing to investigate whether adding pronunciation information may improve the overall quality of the segmentation task. </p>
        <p>We considered two major sources of the pronunciation information for Chinese characters in the Tang dynasty: 
          <span style="font-style:italic">Guangyun</span> and 
          <span style="font-style:italic">Pingshuiyun</span>. 
          <span id="ftn15_return">
            <a class="notelink" title="Guangyun and Pingshuiyun are《廣韻》and《平水韻》, respectively" href="#ftn15">
              <sup>15</sup>
            </a>
          </span> The statistics in Table 1 show that adding pronunciation information into the features did not improve the overall performance for the segmentation task significantly. 
          <span id="ftn16_return">
            <a class="notelink" title="This does not suggest that using the pronunciation information alone was not useful. We have conducted more experiments to evaluate the effectiveness …" href="#ftn16">
              <sup>16</sup>
            </a>
          </span> The results suggest that, given the characters and their bigrams, adding pronunciation did not contribute much more information. Huang et al. (2010) reported similar observations when they used 
          <span style="font-style:italic">Guangyun</span> in their work. Relatively, 
          <span style="font-style:italic">Guangyun</span> is more informative than 
          <span style="font-style:italic">Pingshuiyun</span> for the segmentation tasks.
        </p>
        <div class="table">
          <table class="rules" style="border-collapse:collapse;border-spacing:0;">
            <tr>
              <td style="border: 1px solid black; padding: 2px;">Features</td>
              <td colspan="3" style="border: 1px solid black; padding: 2px;">Width of Context = 1</td>
              <td colspan="3" style="border: 1px solid black; padding: 2px;">Width of Context = 2</td>
            </tr>
            <tr>
              <td style="border: 1px solid black; padding: 2px;"/>
              <td style="border: 1px solid black; padding: 2px;">Precision</td>
              <td style="border: 1px solid black; padding: 2px;">Recall</td>
              <td style="border: 1px solid black; padding: 2px;">F 
                <sub>1</sub>
              </td>
              <td style="border: 1px solid black; padding: 2px;">Precision</td>
              <td style="border: 1px solid black; padding: 2px;">Recall</td>
              <td style="border: 1px solid black; padding: 2px;">F 
                <sub>1</sub>
              </td>
            </tr>
            <tr>
              <td style="border: 1px solid black; padding: 2px;">Characters</td>
              <td style="border: 1px solid black; padding: 2px;">0.652</td>
              <td style="border: 1px solid black; padding: 2px;">0.535</td>
              <td style="border: 1px solid black; padding: 2px;">0.588</td>
              <td style="border: 1px solid black; padding: 2px;">0.695</td>
              <td style="border: 1px solid black; padding: 2px;">0.620</td>
              <td style="border: 1px solid black; padding: 2px;">0.655</td>
            </tr>
            <tr>
              <td style="border: 1px solid black; padding: 2px;">Characters+Bigrams</td>
              <td style="border: 1px solid black; padding: 2px;">0.743</td>
              <td style="border: 1px solid black; padding: 2px;">0.654</td>
              <td style="border: 1px solid black; padding: 2px;">0.696</td>
              <td style="border: 1px solid black; padding: 2px;">0.802</td>
              <td style="border: 1px solid black; padding: 2px;">0.736</td>
              <td style="border: 1px solid black; padding: 2px;">0.768</td>
            </tr>
            <tr>
              <td style="border: 1px solid black; padding: 2px;">Characters+Bigrams+Guangyun</td>
              <td style="border: 1px solid black; padding: 2px;">0.748</td>
              <td style="border: 1px solid black; padding: 2px;">0.671</td>
              <td style="border: 1px solid black; padding: 2px;">0.707</td>
              <td style="border: 1px solid black; padding: 2px;">0.781</td>
              <td style="border: 1px solid black; padding: 2px;">0.707</td>
              <td style="border: 1px solid black; padding: 2px;">0.742</td>
            </tr>
            <tr>
              <td style="border: 1px solid black; padding: 2px;">Characters+Bigrams+Pingshuiyun</td>
              <td style="border: 1px solid black; padding: 2px;">0.737</td>
              <td style="border: 1px solid black; padding: 2px;">0.659</td>
              <td style="border: 1px solid black; padding: 2px;">0.696</td>
              <td style="border: 1px solid black; padding: 2px;">0.763</td>
              <td style="border: 1px solid black; padding: 2px;">0.698</td>
              <td style="border: 1px solid black; padding: 2px;">0.729</td>
            </tr>
          </table>
        </div>
        <p>
          <span style="font-weight:bold">Table 1. Contributions of pronunciation information</span>
        </p>
      </div>
      <div class="DH-Heading2" id="index.xml-body.1_div.3_div.4">
        <h3 class="DH-Heading2">
          <span class="headingNumber">3.4. </span>
          <span class="head">Adding Word-Level Information</span>
        </h3>
        <p>We can obtain information about the reign periods, location names, and office names in the Tang dynasty from CBDB. By segmenting characters for these special words and adding appropriate type information, we added word-level information into the features. The statistics in Table 2 show that the word-level information did not raise the performance very much. 
          <span id="ftn17_return">
            <a class="notelink" title="In Table 2, WOC stands for “Width of Context”, “P” stands for precision, “R” stands for recall, “C+B” stand for “Characters and Bigrams” and “C+B+W” s…" href="#ftn17">
              <sup>17</sup>
            </a>
          </span>
        </p>
        <div class="table">
          <table class="rules" style="border-collapse:collapse;border-spacing:0;">
            <tr>
              <td style="border: 1px solid black; padding: 2px;">Features</td>
              <td colspan="2" style="border: 1px solid black; padding: 2px;">WOC = 1</td>
              <td colspan="2" style="border: 1px solid black; padding: 2px;">WOC = 2</td>
              <td colspan="2" style="border: 1px solid black; padding: 2px;">WOC =3</td>
              <td colspan="3" style="border: 1px solid black; padding: 2px;">WOC = 4</td>
            </tr>
            <tr>
              <td style="border: 1px solid black; padding: 2px;"/>
              <td style="border: 1px solid black; padding: 2px;">P</td>
              <td style="border: 1px solid black; padding: 2px;">R</td>
              <td style="border: 1px solid black; padding: 2px;">P</td>
              <td style="border: 1px solid black; padding: 2px;">R</td>
              <td style="border: 1px solid black; padding: 2px;">P</td>
              <td style="border: 1px solid black; padding: 2px;">R</td>
              <td style="border: 1px solid black; padding: 2px;">P</td>
              <td style="border: 1px solid black; padding: 2px;">R</td>
              <td style="border: 1px solid black; padding: 2px;">F 
                <sub>1</sub>
              </td>
            </tr>
            <tr>
              <td style="border: 1px solid black; padding: 2px;">C+B</td>
              <td style="border: 1px solid black; padding: 2px;">0.743</td>
              <td style="border: 1px solid black; padding: 2px;">0.654</td>
              <td style="border: 1px solid black; padding: 2px;">0.802</td>
              <td style="border: 1px solid black; padding: 2px;">0.736</td>
              <td style="border: 1px solid black; padding: 2px;">0.823</td>
              <td style="border: 1px solid black; padding: 2px;">0.766</td>
              <td style="border: 1px solid black; padding: 2px;">0.839</td>
              <td style="border: 1px solid black; padding: 2px;">0.790</td>
              <td style="border: 1px solid black; padding: 2px;">0.814</td>
            </tr>
            <tr>
              <td style="border: 1px solid black; padding: 2px;">C+B+W</td>
              <td style="border: 1px solid black; padding: 2px;">0.747</td>
              <td style="border: 1px solid black; padding: 2px;">0.671</td>
              <td style="border: 1px solid black; padding: 2px;">0.800</td>
              <td style="border: 1px solid black; padding: 2px;">0.741</td>
              <td style="border: 1px solid black; padding: 2px;">0.818</td>
              <td style="border: 1px solid black; padding: 2px;">0.767</td>
              <td style="border: 1px solid black; padding: 2px;">0.832</td>
              <td style="border: 1px solid black; padding: 2px;">0.787</td>
              <td style="border: 1px solid black; padding: 2px;">0.809</td>
            </tr>
            <tr>
              <td style="border: 1px solid black; padding: 2px;">C+B+PMI</td>
              <td style="border: 1px solid black; padding: 2px;">0.748</td>
              <td style="border: 1px solid black; padding: 2px;">0.661</td>
              <td style="border: 1px solid black; padding: 2px;">0.804</td>
              <td style="border: 1px solid black; padding: 2px;">0.740</td>
              <td style="border: 1px solid black; padding: 2px;">0.824</td>
              <td style="border: 1px solid black; padding: 2px;">0.769</td>
              <td style="border: 1px solid black; padding: 2px;">0.839</td>
              <td style="border: 1px solid black; padding: 2px;">0.791</td>
              <td style="border: 1px solid black; padding: 2px;">0.814</td>
            </tr>
          </table>
        </div>
        <p>
          <span style="font-weight:bold">Table 2. Adding word-level information</span>
        </p>
        <p>We examined the training and test data, and found that, although we gathered the special terms for the Tang dynasty, those words were not used in the biographies often. As a consequence, we did not add a lot of word-level information in the features in reality. </p>
        <p>We have also adopted pointwise mutual information (PMI) of bigrams as features, but the net contributions are not significant. </p>
      </div>
    </div>
    <div class="DH-Heading1" id="index.xml-body.1_div.4">
      <h2 class="DH-Heading1">
        <span class="headingNumber">4. </span>
        <span class="head">Discussions</span>
      </h2>
      <p>We have consulted historians, 
        <sup>6,</sup>
        <span id="ftn18_return">
          <a class="notelink" title="In addition to Hongsu Wang of Harvard University, we also consulted Professor Zhaoquan He (何兆泉) of the China Jiliang University. They use tomb biograp…" href="#ftn18">
            <sup>18</sup>
          </a>
        </span> and learned that our current results are useful in practice. The best precision rates and F measures are better than 0.8 in Figure 3 and Table 2. The best item accuracy is better than 0.94.
      </p>
      <p>In fact, we have designed an advanced mechanism to further improve our results. 
        <span id="ftn19_return">
          <a class="notelink" title="Again, we could not provide details about more experiments because of the word limit for DH 2018 submissions." href="#ftn19">
            <sup>19</sup>
          </a>
        </span> The new approach employs a second level learning step that learns from the errors of the current classifiers.
      </p>
      <p>One may plan to consider more linguistic information in the segmentation tasks. If appropriate corpora or sources are available, it is worthwhile to explore the effects of adding part-of-speech information in the task (Chiu, 2015; Lee, 2012). We have applied deep learning techniques for the segmentation tasks, and achieved better results.</p>
      <p>Although we look for methods to reproduce the segmentations in the given texts, we understand that not all experts will agree upon “the” segmentations for a corpus. Different segmentations may correspond to different interpretations of the texts, especially for the classical Chinese. The results of asking two persons to segment Chinese texts may not match perfectly either (Huang and Chen, 2011).</p>
    </div>
    <!--TEI back-->
    <div class="bibliogr" id="index.xml-back.1_div.1">
      <h2>
        <span class="headingNumber">Appendix A </span>
      </h2>
      <div class="listhead">Bibliography</div>
      <ol class="listBibl">
        <li id="index.xml-bibl-w3045aab3b3b1b1b3">
          <div class="footnote text">Bouma, G. and Adesam, Y. (2013). Experiments on sentence segmentation in Old Swedish editions. 
            <span style="font-style:italic">Proceedings of the Workshop on Computational Historical Linguistics at NODALIDA</span> 2013. NEALT Proceedings Series 18 / Linköping Electronic Conference Proceedings 87:11–26.
          </div>
        </li>
        <li id="index.xml-bibl-w3045aab3b3b1b1b5">
          <div class="footnote text">Chiu, T.-s., Lu, Q., Xu, J., Xiong, D. and Lo, F. (2015). PoS tagging for classical Chinese text. 
            <span style="font-style:italic">Chinese Lexical Semantics</span> ( 
            <span style="font-style:italic">Lecture Notes in Artificial Intelligence</span> 9332), pp. 448–456.
          </div>
        </li>
        <li id="index.xml-bibl-w3045aab3b3b1b1b7">
          <div class="footnote text">Huang, H.-H. and Chen, H.-H. (2011). Pause and stop labeling for Chinese sentence boundary detection. 
            <span style="font-style:italic">Proceedings of the 2011 Conference on Recent Advances in Natural Language Processing</span>, pp. 146‒153.
          </div>
        </li>
        <li id="index.xml-bibl-w3045aab3b3b1b1b9">
          <div class="footnote text">Huang, H.-H., Sun, C.-T., and Chen, H.-H. (2010). Classical Chinese sentence segmentation. 
            <span style="font-style:italic">Proceedings of CIPS-SIGHAN Joint Conference on Chinese Language Processing</span>, pp. 15‒22.
          </div>
        </li>
        <li id="index.xml-bibl-w3045aab3b3b1b1c11">
          <div class="footnote text">Lafferty, J., McCallum, A., and Pereira, F. C.N. (2001). Conditional random fields: Probabilistic models for segmenting and labeling sequence data. 
            <span style="font-style:italic">Proceedings of the Eighteenth International Conference on Machine Learning</span>, pp. 282–289.
          </div>
        </li>
        <li id="index.xml-bibl-w3045aab3b3b1b1c13">
          <div class="footnote text">Lee, J. (2012). A classical Chinese corpus with nested part-of-speech tags. 
            <span style="font-style:italic">Proceedings of the Sixth EACL Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities</span>, pp. 75–84.
          </div>
        </li>
        <li id="index.xml-bibl-w3045aab3b3b1b1c15">
          <div class="footnote text">Petran, F. (2012). Studies for segmentation of historical texts: Sentences or chunks? 
            <span style="font-style:italic">Proceedings of the Second Workshop on Annotation of Corpora for Research in the Humanities</span>, pp. 75–86.
          </div>
        </li>
        <li id="index.xml-bibl-w3045aab3b3b1b1c17">
          <div class="footnote text">Shao, Y., Hardmeier, C., Tiedemann, J., and Nivre, J. (2017). Character-based joint segmentation and POS tagging for Chinese using bidirectional RNN-CRF. 
            <span style="font-style:italic">Proceedings of the 2017 International Joint Conference on Natural Language Processing</span>, pp. 173‒183.
          </div>
        </li>
        <li id="index.xml-bibl-w3045aab3b3b1b1c19">
          <div class="footnote text">Sun, M.-S., Xiao, M. and Tsou, B. K. (2004). Chinese word segmentation without using dictionary based on unsupervised learning strategy. 
            <span style="font-style:italic">Chinese Journal of Computers</span>, 27(6):736‒742. (in Chinese)
          </div>
        </li>
        <li id="index.xml-bibl-w3045aab3b3b1b1c21">
          <div class="footnote text">Wang, B., Shi, X. and Su, J. (2017). A sentence segmentation method for ancient Chinese texts based on recurrent neural network. 
            <span style="font-style:italic">Acta Scientiarum Naturalium Universitatis Pekinensis</span>, 53(2):255‒261. (in Chinese)
          </div>
        </li>
        <li id="index.xml-bibl-w3045aab3b3b1b1c23">
          <div class="footnote text">Wang, B., Shi, X., Tan, Z., Chen, Y. and Wang, W. (2016). A sentence segmentation method for ancient Chinese texts based on NNLM. 
            <span style="font-style:italic">Proceedings of the Chinese Lexical Semantics Workshop</span> 2016, 
            <span style="font-style:italic">Lecture Notes in Computer Science</span> 10085, pp. 387–396.
          </div>
        </li>
        <li id="index.xml-bibl-w3045aab3b3b1b1c25">
          <div class="footnote text">Zhou, S. (周紹良) and Zhao, C. (趙超). (1992). 
            <span style="font-style:italic">A Collection of Tomb Biographies of Tang Dynasty</span> (唐代墓誌彙編). Shanghai Ancient Books Publishing House (上海古籍出版社). (in Chinese)
          </div>
        </li>
        <li id="index.xml-bibl-w3045aab3b3b1b1c27">
          <div class="footnote text">Zhou, S. (周紹良) and Zhao, C. (趙超). (2001). 
            <span style="font-style:italic">A Collection of Tomb Biographies of Tang Dynasty: An Extension</span> (唐代墓誌彙編續集). Shanghai Ancient Books Publishing House. (in Chinese),
          </div>
        </li>
      </ol>
    </div>
    <!--Notes in [TEI]-->
    <div class="notes">
      <div class="noteHeading">Notes</div>
      <div class="note" id="ftn1">
        <span class="noteLabel">1. </span>
        <div class="noteBody">
          <p class="footnote text">This image was downloaded from &lt;http://www.lyqtzz.com/uploadfile/20110817165325665.jpg&gt;. The Tang dynasty existed between 688CE and 907CE. More images of tomb biographies are available at &lt;http://goo.gl/XHCL9P&gt;.</p>
        </div>
      </div>
      <div class="note" id="ftn2">
        <span class="noteLabel">2. </span>
        <div class="noteBody">
          <p class="footnote text">The China Biographical Database (https://projects.iq.harvard.edu/cbdb/home) is a free and open database for Chinese studies.</p>
        </div>
      </div>
      <div class="note" id="ftn3">
        <span class="noteLabel">3. </span>
        <div class="noteBody">
          <p class="footnote text">The 
            <span style="font-weight:bold;color:0000FF">precision</span> rate, 
            <span style="font-weight:bold;color:0000FF">recall</span> rate, and 
            <span style="font-weight:bold;color:0000FF">F measure</span> are designed for evaluating the effectiveness of information retrieval and extraction. F 
            <sub>1</sub> is a popular choice of the F measure.
          </p>
        </div>
      </div>
      <div class="note" id="ftn4">
        <span class="noteLabel">4. </span>
        <div class="noteBody">
          <p class="footnote text">Shiji (史記) and Zuozhuan (左傳) are two very important sources about Chinese history.</p>
        </div>
      </div>
      <div class="note" id="ftn5">
        <span class="noteLabel">5. </span>
        <div class="noteBody">
          <p class="footnote text">The 
            <span style="font-weight:bold;color:0000FF">item accuracy</span> evaluates the labeling judgments including both punctuated and non-punctuated items. In a typical sentence segmentation task, there are many more non-punctuated items than punctuated items, so it is relatively easier to achieve attractive figures for the item accuracy than for the F measure.
          </p>
        </div>
      </div>
      <div class="note" id="ftn6">
        <span class="noteLabel">6. </span>
        <div class="noteBody">
          <p class="footnote text">We interviewed Hongsu Wang (王宏甦), the project manager of the China Biographical Database Project at Harvard University, about his preferences in post-checking the segmentation results that are produced by software. He suggests that higher precision rates are preferred. When seeking higher recall rates (often sacrificing the precision rates), the false-positive recommendations for punctuation are annoying to the researchers.</p>
        </div>
      </div>
      <div class="note" id="ftn7">
        <span class="noteLabel">7. </span>
        <div class="noteBody">
          <p class="footnote text">In terms of Linguistics, we have 5505 character types and 2,461,000 character tokens.</p>
        </div>
      </div>
      <div class="note" id="ftn8">
        <span class="noteLabel">8. </span>
        <div class="noteBody">
          <p class="footnote text">30 is an arbitrary choice, and can be changed easily.</p>
        </div>
      </div>
      <div class="note" id="ftn9">
        <span class="noteLabel">9. </span>
        <div class="noteBody">
          <p class="footnote text">Due to the constraint on the word count in DH 2018 proposals, we can only briefly outline the steps for training and testing CRF models. More details can be provided in the presentation and in an extended report.</p>
        </div>
      </div>
      <div class="note" id="ftn10">
        <span class="noteLabel">10. </span>
        <div class="noteBody">
          <p class="footnote text">Here, we adopt typical notations for CRF-based applications. w[0] is the current word, w[-1] is the neighbor word to the left of the current word, w[1] is the neighbor word to the right of the current word. Two actual instances that are produced from “孝敬天啟，動必以禮” for character-based segmentations will look like the following.</p>
          <p class="footnote text">O w[-1]=敬,w[0]=天,w[1]=啟</p>
          <p class="footnote text">M w[-1]=天,w[0]=啟,w[1]=動</p>
          <p class="footnote text">Two instances that are produced from “母子 忠孝 ， 天下 榮 之” for the word-based segmentations will look like the following.</p>
          <p class="footnote text">M w[-1]=母子,w[0]=忠孝,w[1]=天下 </p>
          <p class="footnote text">O w[-1]=忠孝,w[0]=天下,w[1]=榮</p>
        </div>
      </div>
      <div class="note" id="ftn11">
        <span class="noteLabel">11. </span>
        <div class="noteBody">
          <p class="footnote text">CRFSuite: &lt;http://www.chokkan.org/software/crfsuite/&gt;</p>
        </div>
      </div>
      <div class="note" id="ftn12">
        <span class="noteLabel">12. </span>
        <div class="noteBody">
          <p class="footnote text">Thus, the instances for testing CRFs look like (1) and (2) that do not carry the correct labels, M and O, respectively..</p>
        </div>
      </div>
      <div class="note" id="ftn13">
        <span class="noteLabel">13. </span>
        <div class="noteBody">
          <p class="footnote text">Recall that we used only those biographies that have no less than 30 characters.</p>
        </div>
      </div>
      <div class="note" id="ftn14">
        <span class="noteLabel">14. </span>
        <div class="noteBody">
          <p class="footnote text">Here, w[-1_0] is the bigram on the left side of the current word, and w[0_1] is the bigram to the right of the current word. When we consider bigrams for a wider context, we may consider bigrams like w[-2_-1] and w[1_2].</p>
        </div>
      </div>
      <div class="note" id="ftn15">
        <span class="noteLabel">15. </span>
        <div class="noteBody">
          <p class="footnote text">
            <span style="font-style:italic">Guangyun</span> and 
            <span style="font-style:italic">Pingshuiyun</span> are《廣韻》and《平水韻》, respectively
          </p>
        </div>
      </div>
      <div class="note" id="ftn16">
        <span class="noteLabel">16. </span>
        <div class="noteBody">
          <p class="footnote text">This does not suggest that using the pronunciation information alone was not useful. We have conducted more experiments to evaluate the effectiveness of using the pronunciation information for the segmentation tasks, and will provide more details in the presentation and in an extended report.</p>
        </div>
      </div>
      <div class="note" id="ftn17">
        <span class="noteLabel">17. </span>
        <div class="noteBody">
          <p class="footnote text">In Table 2, WOC stands for “Width of Context”, “P” stands for precision, “R” stands for recall, “C+B” stand for “Characters and Bigrams” and “C+B+W” stands for “Characters, Bigrams, and Words”.</p>
        </div>
      </div>
      <div class="note" id="ftn18">
        <span class="noteLabel">18. </span>
        <div class="noteBody">
          <p class="footnote text">In addition to Hongsu Wang of Harvard University, we also consulted Professor Zhaoquan He (何兆泉) of the China Jiliang University. They use tomb biographies of the Tang and the Song dynasties in their research. </p>
        </div>
      </div>
      <div class="note" id="ftn19">
        <span class="noteLabel">19. </span>
        <div class="noteBody">
          <p class="footnote text">Again, we could not provide details about more experiments because of the word limit for DH 2018 submissions.</p>
        </div>
      </div>
    </div>
    <div class="stdfooter autogenerated">
      <address>Chao-Lin Liu (chaolinliu@gmail.com), National Chengchi University, Taiwan and Yi Chang (black.heptagram@gmail.com), National Chengchi University, Taiwan</address>
    </div>
  </body>
</html>
