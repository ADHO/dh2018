<?xml version="1.0" encoding="UTF-8"?>
<html xml:lang="en" xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
    <!--THIS FILE IS GENERATED FROM AN XML MASTER. DO NOT EDIT (5)-->
    <title>Negentropic linguistic evolution: A comparison of seven languages</title>
    <meta name="author" content="Vincent Buntinx and Frédéric Kaplan"/>
    <meta name="generator" content="Text Encoding Initiative Consortium XSLT stylesheets"/>
    <meta name="DC.Title" content="Negentropic linguistic evolution: A comparison of seven languages"/>
    <meta name="DC.Type" content="Text"/>
    <meta name="DC.Format" content="text/html"/>
    <link href="http://www.tei-c.org/release/xml/tei/stylesheet/tei.css" rel="stylesheet" type="text/css"/>
    <link rel="stylesheet" media="print" type="text/css" href="http://www.tei-c.org/release/xml/tei/stylesheet/tei-print.css"/>
  </head>
  <body class="simple" id="TOP">
    <div class="stdheader autogenerated">
      <h1 class="maintitle">Negentropic linguistic evolution: A comparison of seven languages</h1>
    </div>
    <div class="dhconvalidator-xml-link">
      <a href="BUNTINX_Vincent_Christian_Negentropic_linguistic_evolution__.xml">XML</a>
    </div>
    <!--TEI front-->
    <!--TEI body-->
    <p class="Plain Text">Introduction</p>
    <p class="Plain Text">The relationship between the entropy of language and its complexity has been the subject of much speculation – some seeing the increase of linguistic entropy as a sign of linguistic complexification or interpreting entropy drop as a marker of greater regularity (Montemurro and Zanette 2011, Juola 2016, Bentz et al. 2017). Some evolutionary explanations, like the learning bottleneck hypothesis, argues that communication systems having more regular structures tend to have evolutionary advantages over more complex structures (Kirby 2001, Tamariz and Kirby 2016, Ferrer I Cancho 2017). Other structural effects of communication networks, like globalization of exchanges or algorithmic mediation, have been hypothesized to have a regularization effect on language (Kaplan 2014). </p>
    <p class="Plain Text">Longer-term studies are now possible thanks to the arrival of large-scale diachronic corpora, like newspaper archives or digitized libraries (Westin and Geisler 2002, Fries and Lehmann 2006, Lyse and Andersen 2012, Rochat et al. 2016). However, simple analyses of such datasets are prone to misinterpretations due to significant variations of corpus size over the years and the indirect effect this can have on various measures of language change and linguistic complexity (Buntinx et al. 2017). In particular, it is important not to misinterpret the arrival of new words as an increase in complexity as this variation is intrinsical, as is the variation of corpus size.</p>
    <p class="Plain Text">This paper is an attempt to conduct an unbiased diachronic study of linguistic complexity over seven different languages using the Google Books corpus (Michel et al. 2011). The paper uses a simple entropy measure on a closed, but nevertheless large, subset of words, called kernels (Buntinx et al. 2016). The kernel contains only the words that are present without interruption for the whole length of the study. This excludes all the words that arrived or disappeared during the period. We argue that this method is robust towards variations of corpus size and permits to study change in complexity despite possible (and in the case of Google Books unknown) change in the composition of the corpus. Indeed, the evolution observed on the seven different languages shows rather different patterns that are not directly correlated with the evolution of the size of the respective corpora. The rest of the paper presents the methods followed, the results obtained and the next steps we envision.</p>
    <p class="Plain Text">Method and Results</p>
    <p class="Plain Text">We use the concept of kernel entropy (Buntinx et al. 2017), defined as the Shannon entropy measure applied on word occurrences distribution normalized on the kernel of a given corpus. To calculate this measure, the corpus is subdivided into yearly sub-corpora. Next, we then calculate the word occurrences for the words that are present in each sub-corpus for each year. These words form a set, called a kernel. The word frequencies are normalized on the kernel 
      <math xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>K</mi>
      </math> for each year 
      <math xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>y</mi>
      </math> and the formula of Shannon entropy (using napierian logarithm) is applied on these distributions providing a measure that can be compared diachronically with robustness to corpus size evolution and to noises. The kernel entropy of a kernel 
      <math xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>K</mi>
      </math> for the year 
      <math xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>y</mi>
      </math> is given by the formula:
    </p>
    <div class="figure">
      <img src="Pictures/9382f9d785310041029d0a0f99312472.png" alt="" class="inline" style=" width:5.2535694444444445cm; height:1.7991666666666666cm;"/>
    </div>
    <p class="No Spacing">Where 
      <math xmlns="http://www.w3.org/1998/Math/MathML">
        <msup>
          <mrow>
            <mi>N</mi>
          </mrow>
          <mrow>
            <mi>K</mi>
          </mrow>
        </msup>
      </math> is the number of words composing the kernel and 
      <math xmlns="http://www.w3.org/1998/Math/MathML">
        <msubsup>
          <mrow>
            <mi>f</mi>
          </mrow>
          <mrow>
            <mi>i</mi>
          </mrow>
          <mrow>
            <mi>K</mi>
            <mo>,</mo>
            <mi>y</mi>
          </mrow>
        </msubsup>
      </math> the relative occurrence frequency of the word 
      <math xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>i</mi>
      </math> normalized on the kernel 
      <math xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>K</mi>
      </math> in the year 
      <math xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>y</mi>
      </math>. The kernel entropy measure is computed for seven languages of Google Books corpora. 
      <span style="font-style:italic">Figure 1</span> shows the kernel entropy variations normalized with respect to the average value (which change over the languages because kernels of different corpus also have different sizes).
    </p>
    <div class="figure">
      <img src="Pictures/c82342d8bfd9d6f4d2aa53ca9b3e2c44.png" alt="" class="inline" style=" width:16.59113888888889cm; height:10.851319444444444cm;"/>
    </div>
    <p class="No Spacing">Figure 1: Normalized yearly kernel entropy evolution from 1800 to 2009 of seven Google Books corpora: British English, American English, French, German, Italian, Spanish and Russian.</p>
    <p class="No Spacing">We observe that even if all the seven language have different patterns and inflection points, they tend generally to show an effect of negentropy with increasing years. We note that most languages have a crosspoint in 1905, except for the Russian language, showing variations particularly from 1920 to 1930. We present in Figure 2 the kernel entropy evolution for each language in comparison to the corpus size.</p>
    <div class="table">
      <table class="rules" style="border-collapse:collapse;border-spacing:0;">
        <tr>
          <td class="No_Spacing">
            <div class="figure">
              <img src="Pictures/7b52952f7cacae749a03fa7476bc5823.png" alt="" class="inline" style=" width:8cm; height:5.23cm;"/>
            </div>
          </td>
          <td class="No_Spacing">
            <div class="figure">
              <img src="Pictures/5d5242dd47387cc86d72fcbd120df78d.png" alt="" class="inline" style=" width:8cm; height:5.23cm;"/>
            </div>
          </td>
        </tr>
        <tr>
          <td class="No_Spacing">
            <div class="figure">
              <img src="Pictures/44c2bb28b891812dd67889713eb09444.png" alt="" class="inline" style=" width:8cm; height:5.23cm;"/>
            </div>
          </td>
          <td class="No_Spacing">
            <div class="figure">
              <img src="Pictures/a9231b34e46635f6c849b195b13ea640.png" alt="" class="inline" style=" width:8cm; height:5.23cm;"/>
            </div>
          </td>
        </tr>
        <tr>
          <td class="No_Spacing">
            <div class="figure">
              <img src="Pictures/269c781015d6cc5c0781a55e16246a78.png" alt="" class="inline" style=" width:8cm; height:5.23cm;"/>
            </div>
          </td>
          <td class="No_Spacing">
            <div class="figure">
              <img src="Pictures/8dbd3e0ba863083cc8823e3e2183b5ab.png" alt="" class="inline" style=" width:8cm; height:5.23cm;"/>
            </div>
          </td>
        </tr>
        <tr>
          <td class="No_Spacing">
            <p class="No Spacing">Legend:</p>
            <p class="No Spacing">
              <span style="font-weight:bold">(1) British / American English</span>
            </p>
            <p class="No Spacing">
              <span style="font-weight:bold">(2) French / Italian</span>
            </p>
            <p class="No Spacing">
              <span style="font-weight:bold">(3) Spanish / German</span>
            </p>
            <p class="No Spacing">
              <span style="font-weight:bold">(4) Russian</span>
            </p>
            <p class="No Spacing">
              <span style="font-weight:bold;color:0070C0">Kernel Entropy: Blue</span>
            </p>
            <p class="No Spacing">
              <span style="font-weight:bold;color:FF0000">Size: Red</span>
            </p>
          </td>
          <td class="No_Spacing">
            <div class="figure">
              <img src="Pictures/d225ad506ffb4dc7f94c08c35db5a165.png" alt="" class="inline" style=" width:8cm; height:5.23cm;"/>
            </div>
          </td>
        </tr>
      </table>
    </div>
    <p class="No Spacing">Figure 2: Yearly kernel entropy evolution and size evolution from 1800 to 2009 of seven Google Books corpora: British English, American English, French, German, Italian, Spanish and Russian.</p>
    <p>Google Books corpora may experience sudden changes in composition depending on the year. For example, the addition of scientific literature and medical journals (Pechenick et al., 2015). In this case, the words kernel distribution, even if it is robust because composed of the most stable words, can change for a year which is subject to a change of composition of the corpus. However, this effect is still reduced because the words appearing and disappearing during this transition phase are not taken into account. We observe that the entropy of the kernel seems not to be affected by the size variations of corpora and when it appear to be affected, the direction of variation is unpredictable.</p>
    <p>The British English and American English are the least affected languages by the negentropic effect. Their kernel entropy increases over time until 1960 (British English) and 1940 (American English). However, American English kernel entropy decrease quickly from 1940 to 1985. We observe that the obtained curve for the French language is similar to the one corresponding to the study of language evolution through 200 years of newspapers written in French despite a different kernel size (Buntinx et al. 2017). </p>
    <p>Interesting inflection points are detected and should be poignant to specialists of the targeted language. We present in 
      <span style="font-style:italic">Figure 3</span> the number of words in the kernel and inflections points for the seven languages.
    </p>
    <div class="table">
      <table class="rules" style="border-collapse:collapse;border-spacing:0;">
        <tr>
          <td style="border: 1px solid black; padding: 2px;">Language</td>
          <td style="border: 1px solid black; padding: 2px;">Number of words in the kernel</td>
          <td style="border: 1px solid black; padding: 2px;">Inflection point 1</td>
          <td style="border: 1px solid black; padding: 2px;">Inflection point 2</td>
        </tr>
        <tr>
          <td style="border: 1px solid black; padding: 2px;">British English</td>
          <td style="border: 1px solid black; padding: 2px;">82’332</td>
          <td style="border: 1px solid black; padding: 2px;">1959</td>
          <td style="border: 1px solid black; padding: 2px;"/>
        </tr>
        <tr>
          <td style="border: 1px solid black; padding: 2px;">American English</td>
          <td style="border: 1px solid black; padding: 2px;">44’949</td>
          <td style="border: 1px solid black; padding: 2px;">1931</td>
          <td style="border: 1px solid black; padding: 2px;">1985</td>
        </tr>
        <tr>
          <td style="border: 1px solid black; padding: 2px;">French</td>
          <td style="border: 1px solid black; padding: 2px;">79’575</td>
          <td style="border: 1px solid black; padding: 2px;">1825</td>
          <td style="border: 1px solid black; padding: 2px;">1885</td>
        </tr>
        <tr>
          <td style="border: 1px solid black; padding: 2px;">German</td>
          <td style="border: 1px solid black; padding: 2px;">36’660</td>
          <td style="border: 1px solid black; padding: 2px;">1850</td>
          <td style="border: 1px solid black; padding: 2px;">1946</td>
        </tr>
        <tr>
          <td style="border: 1px solid black; padding: 2px;">Italian</td>
          <td style="border: 1px solid black; padding: 2px;">30’996</td>
          <td style="border: 1px solid black; padding: 2px;">1983</td>
          <td style="border: 1px solid black; padding: 2px;"/>
        </tr>
        <tr>
          <td style="border: 1px solid black; padding: 2px;">Spanish</td>
          <td style="border: 1px solid black; padding: 2px;">25’582</td>
          <td style="border: 1px solid black; padding: 2px;">1995</td>
          <td style="border: 1px solid black; padding: 2px;"/>
        </tr>
        <tr>
          <td style="border: 1px solid black; padding: 2px;">Russian</td>
          <td style="border: 1px solid black; padding: 2px;">5’123</td>
          <td style="border: 1px solid black; padding: 2px;">1920</td>
          <td style="border: 1px solid black; padding: 2px;">1988</td>
        </tr>
      </table>
    </div>
    <p class="No Spacing">Figure 3: Number of words in the kernel and kernel entropy inflection points for the seven Google Books corpora: British English, American English, French, German, Italian, Spanish and Russian.</p>
    <p>Furthermore, it is possible to show the languages proximity in terms of kernel entropy evolution behavior through the determination of a distance based on kernel entropy correlations. A projection of the resulting matrix distance using PCA is presented in Figure 4.</p>
    <p class="Plain Text">We observe that British English and American English are represented together to the left of the plan because they have a relative opposite pattern with respect to other languages. Russian is also particular because of the brutal effect of the negentropy observed between around 1920 and the sudden increase at the end of the 1980s. The last four languages, French, Spanish, German and Italian share a more similar behavior and are represented in the right-bottom part of the plan. </p>
    <p class="Plain Text">Although much more in-depth investigation must be done, it is reasonable to make the hypothesis of different internal and external factors for explaining these various patterns. The Russian case clearly invites to investigate correlations between linguistic policies during the Sovietic period and their actual effects of the Russian language.</p>
    <p class="Plain Text">The similarity between French, German, Italian and Spanish pushes in the direction for similar processes of standardization, potentially due to linguistic convergence at national levels suppressing some regional particularities. In contrast, American and British English evolution is likely to be explained through the particular histories of the respective English-speaking populations and their relation to the rest of world. The progressive rise of English as a global language, spoken and written by many non-native speakers, is certainly playing a role in the shaping these particular curves. </p>
    <div class="figure">
      <img src="Pictures/20159d811a1630a94bf5b3e732e5273d.png" alt="" class="inline" style=" width:14.515041666666667cm; height:10.489847222222222cm;"/>
    </div>
    <p class="No Spacing">Figure 4: PCA projection of distance matrix using kernel entropy correlation-based distance for Google Books corpora: British English, US English, French, German, Italian, Spanish and Russian.</p>
    <!--TEI back-->
    <div class="bibliogr" id="index.xml-back.1_div.1">
      <h2>
        <span class="headingNumber">Appendix A </span>
      </h2>
      <div class="listhead">Bibliography</div>
      <ol class="listBibl">
        <li id="index.xml-bibl-w1565aab3b3b1b1b3">
          <div class="No Spacing">C. Bentz, D. Alikaniotis, M. Cysouw and R. Ferrer-i-Cancho. The Entropy of Words—Learnability and Expressivity across More Than 1000 Languages. Entropy, 19(6), 275, 2017.</div>
        </li>
        <li id="index.xml-bibl-w1565aab3b3b1b1b5">
          <div class="No Spacing">V. Buntinx, C. Bornet and F. Kaplan. Studying Linguistic Changes on 200 Years of Newspapers. 
            <span style="font-style:italic">DH2016</span>, Kraków, Poland, July 11-16, 2016.
          </div>
        </li>
        <li id="index.xml-bibl-w1565aab3b3b1b1b7">
          <div class="No Spacing">V. Buntinx 
            <span class="infoscience_authors">, </span>F. Kaplan 
            <span class="infoscience_authors">and </span>A. Xanthos 
            <span class="infoscience_authors">(Dirs.).</span> Analyse multi-échelle de n-grammes sur 200 années d'archives de presse. Thèse EPFL, n° 8180, 2017.
          </div>
        </li>
        <li id="index.xml-bibl-w1565aab3b3b1b1b9">
          <div class="No Spacing">R. Ferrer-i-Cancho. Optimization models of natural communication. Journal of Quantitative Linguistics, 1-31, 2017.</div>
        </li>
        <li id="index.xml-bibl-w1565aab3b3b1b1c11">
          <div class="No Spacing">U. Fries and H. M. Lehmann. The style of 18th century english newspapers: Lexical diversity. News Discourse in Early Modern Britain, pages 91–104, 2006.</div>
        </li>
        <li id="index.xml-bibl-w1565aab3b3b1b1c13">
          <div class="No Spacing">P. Juola. Using the Google N-Gram corpus to measure cultural complexity. Literary and linguistic computing, 28(4), 668-675, 2013.</div>
        </li>
        <li id="index.xml-bibl-w1565aab3b3b1b1c15">
          <div class="No Spacing">F. Kaplan. Linguistic capitalism and algorithmic mediation. Representations, 127 (1):57–63, 2014.</div>
        </li>
        <li id="index.xml-bibl-w1565aab3b3b1b1c17">
          <div class="No Spacing">S. Kirby. Spontaneous evolution of linguistic structure-an iterated learning model of the emergence of regularity and irregularity. IEEE Transactions on Evolutionary Computation, vol. 5, no 2, p. 102-110, 2001.</div>
        </li>
        <li id="index.xml-bibl-w1565aab3b3b1b1c19">
          <div class="No Spacing">G. I. Lyse and G. Andersen. Collocations and statistical analysis of n-grams. Exploring Newspaper Language: Using the Web to Create and Investigate a Large Corpus of Modern Norwegian. Studies in Corpus Linguistics, John Benjamins Publishing, Amsterdam, pages 79–109, 2012.</div>
        </li>
        <li id="index.xml-bibl-w1565aab3b3b1b1c21">
          <div class="No Spacing">J. B. Michel, Y. K. Shen, A. P. Aiden, A. Veres, M. K. Gray, J. P. Pickett, 
            <span class="name">D. Hoiberg, D. Clancy, P. Norvig, J. Orwant, S. Pinker, M. A. Nowak</span> and E. Lieberman-Aiden. Quantitative analysis of culture using millions of digitized books. 
            <span style="font-style:italic">science</span>, 
            <span style="font-style:italic">331</span>(6014), 176-182, 2011.
          </div>
        </li>
        <li id="index.xml-bibl-w1565aab3b3b1b1c23">
          <div class="No Spacing">M. A. Montemurro and D. H. Zanette. Universal entropy of word ordering across linguistic families. PLoS One, 6(5), e19875, 2011.</div>
        </li>
        <li id="index.xml-bibl-w1565aab3b3b1b1c25">
          <div class="No Spacing">E. A. Pechenick, C. M. Danforth and P. S. Dodds. Characterizing the Google Books corpus: Strong limits to inferences of socio-cultural and linguistic evolution. PloS one, 10(10), e0137041, 2015.</div>
        </li>
        <li id="index.xml-bibl-w1565aab3b3b1b1c27">
          <div class="No Spacing">Y. Rochat, M. Ehrmann, V. Buntinx, C. Bornet and F. Kaplan. Navigating through 200 years of historical newspapers. In iPRES 2016, numéro EPFL-CONF-218707, 2016.</div>
        </li>
        <li id="index.xml-bibl-w1565aab3b3b1b1c29">
          <div class="No Spacing">M. Tamariz and S. Kirby. The cultural evolution of language. Current Opinion in Psychology 8: 37-43, 2016.</div>
        </li>
        <li id="index.xml-bibl-w1565aab3b3b1b1c31">
          <div class="No Spacing">I. Westin and C. Geisler. A multi-dimensional study of diachronic variation in british newspaper editorials. International Computer Archive of Modern and Medieval English, (26):133–152, 2002.</div>
        </li>
      </ol>
    </div>
    <div class="stdfooter autogenerated">
      <address>Vincent Buntinx (vincent.buntinx@epfl.ch), EPFL (École polytechnique fédérale de Lausanne), Switzerland and Frédéric Kaplan (frederic.kaplan@epfl.ch), EPFL (École polytechnique fédérale de Lausanne), Switzerland</address>
    </div>
  </body>
</html>
